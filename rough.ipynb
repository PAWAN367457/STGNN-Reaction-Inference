{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1363618",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03aff832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.        ]\n",
      " [-0.0070577   0.00705771  0.00705771]\n",
      " [-0.01242034  0.01242027  0.01242027]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from stgnn_model import STGNNModel\n",
    "\n",
    "model = STGNNModel()\n",
    "model.load_state_dict(torch.load(\"stgnn_stage2.pt\", map_location=\"cpu\"),strict=False)\n",
    "\n",
    "bias = model.stgnn.gat.bias.detach().cpu().numpy()\n",
    "print(bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd570407",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.        ]\n",
      " [-0.02424086  0.23003557 -0.23088887]\n",
      " [-0.01894718 -0.22278465  0.22931455]]\n"
     ]
    }
   ],
   "source": [
    "bias = model.stgnn.gat.bias.detach().cpu().numpy()\n",
    "print(bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "158c22db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.        ]\n",
      " [-0.01587975  0.01587967  0.01587967]\n",
      " [-0.01696245  0.01696243  0.01696243]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from stgnn_model import STGNNModel\n",
    "\n",
    "model = STGNNModel()\n",
    "model.load_state_dict(torch.load(\"stgnn_reaction.pt\", map_location=\"cpu\"),strict= False)\n",
    "\n",
    "bias = model.stgnn.gat.bias.detach().cpu().numpy()\n",
    "print(bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "95145035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved 621-dim motion: reaction_inference/clip_0_listener_future_621.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "# =====================\n",
    "# PATHS\n",
    "# =====================\n",
    "INPUT_181 = \"reaction_inference/clip_0_listener_future_181.npy\"\n",
    "OUTPUT_621 = \"reaction_inference/clip_0_listener_future_621.npy\"\n",
    "NORM_STATS = \"/home/mudasir/Pawan/MPII/stacked_npy/checkpoints_tokenizer/norm_stats.pt\"\n",
    "\n",
    "# =====================\n",
    "# Load data\n",
    "# =====================\n",
    "motion = np.load(INPUT_181).astype(np.float32)  # (2,150,181)\n",
    "\n",
    "norm = torch.load(NORM_STATS, map_location=\"cpu\")\n",
    "mean = norm[\"mean\"]\n",
    "std = norm[\"std\"]\n",
    "\n",
    "# pad mean/std to 181\n",
    "if mean.shape[0] < 181:\n",
    "    pad = 181 - mean.shape[0]\n",
    "    mean = torch.cat([mean, torch.zeros(pad)])\n",
    "    std = torch.cat([std, torch.ones(pad)])\n",
    "\n",
    "mean = mean.numpy()\n",
    "std = std.numpy()\n",
    "\n",
    "# =====================\n",
    "# Denormalize\n",
    "# =====================\n",
    "motion_denorm = motion * std + mean   # (2,150,181)\n",
    "\n",
    "# =====================\n",
    "# Convert 181 ‚Üí 621\n",
    "# =====================\n",
    "B, T, D = motion_denorm.shape\n",
    "\n",
    "expression = motion_denorm[:, :, :177]\n",
    "rotation = np.zeros((B, T, 3), dtype=np.float32)\n",
    "\n",
    "shape = np.zeros((B, T, 156), dtype=np.float32)\n",
    "texture = np.zeros((B, T, 251), dtype=np.float32)\n",
    "lighting = np.zeros((B, T, 27), dtype=np.float32)\n",
    "translation = np.tile(np.array([0, 0, 1], dtype=np.float32), (B, T, 1))\n",
    "eye = np.zeros((B, T, 4), dtype=np.float32)\n",
    "\n",
    "full_621 = np.concatenate([\n",
    "    shape,\n",
    "    expression,\n",
    "    texture,\n",
    "    lighting,\n",
    "    rotation,\n",
    "    translation,\n",
    "    eye\n",
    "], axis=2)\n",
    "\n",
    "assert full_621.shape[2] == 621\n",
    "\n",
    "np.save(OUTPUT_621, full_621)\n",
    "print(\"‚úÖ Saved 621-dim motion:\", OUTPUT_621)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a833b79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved:\n",
      "clip_0_listener1_621.npy (150, 181)\n",
      "clip_0_listener2_621.npy (150, 181)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "\n",
    "inp = \"/home/mudasir/Pawan/MPII/simple_tranformer/STGNN/reaction_inference/recording13_clip20_listener_future_181.npy\"\n",
    "out_dir = \"/home/mudasir/Pawan/MPII/simple_tranformer/STGNN/reaction_inference\"\n",
    "os.makedirs(out_dir, exist_ok=True)\n",
    "\n",
    "data = np.load(inp)  # (2, 150, 621)\n",
    "\n",
    "np.save(os.path.join(out_dir, \"clip_0_listener1_621.npy\"), data[0])\n",
    "np.save(os.path.join(out_dir, \"clip_0_listener2_621.npy\"), data[1])\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\"clip_0_listener1_621.npy\", data[0].shape)\n",
    "print(\"clip_0_listener2_621.npy\", data[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08b46dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded normalized prediction: (102, 181)\n",
      "Mean/std shape after padding: (181,) (181,)\n",
      "Denormalized motion shape: (102, 181)\n",
      "Value range: -0.293263 0.8504865\n",
      "Expression shape: (102, 177)\n",
      "Rotation shape: (102, 3)\n",
      "‚úÖ Final FaceVerse format: (102, 621)\n",
      "Final value range: -0.293263 1.0\n",
      "üíæ Saved: /home/mudasir/Pawan/MPII/simple_tranformer/STGNN/reaction_inference/single_listener_0_future_621.npy\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# === INPUT / OUTPUT ===\n",
    "input_path = \"/home/mudasir/Pawan/MPII/simple_tranformer/STGNN/MPII_GroupReaction_Clean/motion/recording07/clip1/speaker.npy\"\n",
    "output_path = \"/home/mudasir/Pawan/MPII/simple_tranformer/STGNN/reaction_inference/single_listener_0_future_621.npy\"\n",
    "\n",
    "# === NORMALIZATION STATS ===\n",
    "norm_stats_path = \"/home/mudasir/Pawan/MPII/stacked_npy/checkpoints_tokenizer/norm_stats.pt\"\n",
    "\n",
    "recon_norm = np.load(input_path).astype(np.float32)\n",
    "T, D = recon_norm.shape\n",
    "\n",
    "print(f\"Loaded normalized prediction: {recon_norm.shape}\")\n",
    "\n",
    "norm = torch.load(norm_stats_path, map_location=\"cpu\")\n",
    "mean = norm[\"mean\"]     # (177,)\n",
    "std  = norm[\"std\"]      # (177,)\n",
    "\n",
    "# Pad to 181 dims (tokenizer input)\n",
    "pad = 181 - mean.shape[0]\n",
    "if pad > 0:\n",
    "    mean = torch.cat([mean, torch.zeros(pad)])\n",
    "    std  = torch.cat([std, torch.full((pad,), 1e-8)])\n",
    "\n",
    "mean = mean.numpy()\n",
    "std  = std.numpy()\n",
    "\n",
    "print(\"Mean/std shape after padding:\", mean.shape, std.shape)\n",
    "\n",
    "recon_raw = recon_norm * std + mean\n",
    "\n",
    "print(\"Denormalized motion shape:\", recon_raw.shape)\n",
    "print(\"Value range:\", recon_raw.min(), recon_raw.max())\n",
    "\n",
    "if D >= 177:\n",
    "    expression = recon_raw[:, :177]\n",
    "else:\n",
    "    raise ValueError(f\"Expected >=177 dims, got {D}\")\n",
    "\n",
    "print(\"Expression shape:\", expression.shape)\n",
    "\n",
    "rotation = np.zeros((T, 3), dtype=np.float32)\n",
    "\n",
    "if D > 177:\n",
    "    extra = recon_raw[:, 177:181]\n",
    "    if extra.shape[1] >= 3:\n",
    "        rotation[:, :3] = extra[:, :3]\n",
    "\n",
    "print(\"Rotation shape:\", rotation.shape)\n",
    "\n",
    "shape = np.zeros((T, 156), dtype=np.float32)\n",
    "texture = np.zeros((T, 251), dtype=np.float32)\n",
    "lighting = np.zeros((T, 27), dtype=np.float32)\n",
    "\n",
    "translation = np.tile(\n",
    "    np.array([0.0, 0.0, 1.0], dtype=np.float32),\n",
    "    (T, 1)\n",
    ")\n",
    "\n",
    "eye_rot = np.zeros((T, 4), dtype=np.float32)\n",
    "\n",
    "full_621 = np.concatenate([\n",
    "    shape,        # 0‚Äì155   (156)\n",
    "    expression,   # 156‚Äì332 (177)\n",
    "    texture,      # 333‚Äì583 (251)\n",
    "    lighting,     # 584‚Äì610 (27)\n",
    "    rotation,     # 611‚Äì613 (3)\n",
    "    translation,  # 614‚Äì616 (3)\n",
    "    eye_rot       # 617‚Äì620 (4)\n",
    "], axis=1)\n",
    "\n",
    "assert full_621.shape == (T, 621), f\"‚ùå Wrong shape: {full_621.shape}\"\n",
    "\n",
    "print(f\"‚úÖ Final FaceVerse format: {full_621.shape}\")\n",
    "print(\"Final value range:\", full_621.min(), full_621.max())\n",
    "\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "np.save(output_path, full_621)\n",
    "\n",
    "print(f\"üíæ Saved: {output_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62ad1d9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 frames ‚Üí creating video...\n",
      "MoviePy - Building video /home/mudasir/Pawan/FaceVerse_v4/renders/recording13_clip5_listener.mp4.\n",
      "MoviePy - Writing video /home/mudasir/Pawan/FaceVerse_v4/renders/recording13_clip5_listener.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready /home/mudasir/Pawan/FaceVerse_v4/renders/recording13_clip5_listener.mp4\n",
      "DONE! Video saved: /home/mudasir/Pawan/FaceVerse_v4/renders/recording13_clip5_listener.mp4\n",
      "Open it now ‚Äî your face is ALIVE.\n"
     ]
    }
   ],
   "source": [
    "# make_video_moviepy.py\n",
    "from moviepy.video.io.ImageSequenceClip import ImageSequenceClip\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Update these paths\n",
    "frame_folder = \"/home/mudasir/Pawan/FaceVerse_v4/renders/recording13_clip5_listener\"  # your output folder\n",
    "output_video = \"/home/mudasir/Pawan/FaceVerse_v4/renders/recording13_clip5_listener.mp4\"\n",
    "\n",
    "# Get all frames in order\n",
    "frames = sorted(glob.glob(os.path.join(frame_folder, \"*frame_*.jpg\")))\n",
    "# or: frames = sorted(glob.glob(f\"{frame_folder}/*.jpg\"))\n",
    "\n",
    "print(f\"Found {len(frames)} frames ‚Üí creating video...\")\n",
    "\n",
    "# Create video (30 FPS, high quality)\n",
    "clip = ImageSequenceClip(frames, fps=30)\n",
    "clip.write_videofile(\n",
    "    output_video,\n",
    "    codec=\"libx264\",\n",
    "    bitrate=\"5000k\",\n",
    "    preset=\"slow\",\n",
    "    ffmpeg_params=[\"-pix_fmt\", \"yuv420p\"]  # Chrome/Safari compatible\n",
    ")\n",
    "\n",
    "print(f\"DONE! Video saved: {output_video}\")\n",
    "print(\"Open it now ‚Äî your face is ALIVE.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b7fd68",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "alm2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
